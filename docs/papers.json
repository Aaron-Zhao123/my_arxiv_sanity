{
  "papers": [
    {
      "name": "Pimba: A Processing-in-Memory Acceleration for Post-Transformer Large Language Model Serving",
      "arxiv_id": "2507.10178",
      "summary": "The paper proposes Pimba, a system for scalable and efficient serving of large language models (LLMs), especially those using post-Transformer architectures. It addresses memory bandwidth bottlenecks via algorithm-hardware co-design and explores quantization and other strategies to optimize both performance and deployment. The work is central to system-level efficiency and practical LLM infrastructure.",
      "authors": "Not specified in excerpt"
    },
    {
      "name": "MC$^2$A: Enabling Algorithm-Hardware Co-Design for Efficient Markov Chain Monte Carlo Acceleration",
      "arxiv_id": "2507.12935",
      "summary": "MC$^2$A demonstrates an efficient, scalable algorithm-hardware co-design for accelerating Markov Chain Monte Carlo (MCMC), critical for ML inference. It emphasizes end-to-end efficiency and evaluates real-world workloads, providing a framework for bridging system/algorithm interfaceâ€”a key aspect of modern scalable ML system design.",
      "authors": "Not specified in excerpt"
    },
    {
      "name": "Characterizing State Space Model (SSM) and SSM-Transformer Hybrid Language Model Performance with Long Context Length",
      "arxiv_id": "2507.12442",
      "summary": "This paper benchmarks and analyzes inference performance, scaling, and hardware bottlenecks for SSM and hybrid SSM-Transformer language models over long context lengths. The work provides insights into hardware-aware evaluation, informing future system/algorithm co-design for efficient deployment and optimization of new ML paradigms.",
      "authors": "Not specified in excerpt"
    },
    {
      "name": "Elk: Exploring the Efficiency of Inter-core Connected AI Chips with Deep Learning Compiler Techniques",
      "arxiv_id": "2507.11506",
      "summary": "Elk explores infrastructure-level efficiency through examining inter-core connectivity of AI chips, combined with innovations in deep learning compiler techniques. The work is relevant for optimizing data center-scale AI and LLM inference deployment, particularly at the intersection of infrastructure and software/hardware co-design.",
      "authors": "Not specified in excerpt"
    }
  ],
  "last_updated": "2025-07-18T13:32:52.999926",
  "count": 4
}