# ðŸ“š New Research Papers

*Latest papers from arXiv based on your research interests*

**Last Updated:** 2025-07-18 01:42:38  
**Total Papers:** 3

---


## 1. MC$^2$A: Enabling Algorithm-Hardware Co-Design for Efficient Markov Chain Monte Carlo Acceleration

**Authors:** Shirui Zhao et al.

[ðŸ“„ View on arXiv](https://arxiv.org/abs/2507.12935v1)

### Abstract
Proposes a system-level algorithm-hardware co-design for efficient acceleration of Markov Chain Monte Carlo (MCMC) methods, relevant for improving the efficiency of ML models. MC$^2$A introduces a flexible, general hardware architecture optimized for a range of MCMC workloads, addressing real-world deployment and scalable infrastructure for ML systems.

---


## 2. Characterizing State Space Model (SSM) and SSM-Transformer Hybrid Language Model Performance with Long Context Length

**Authors:** Saptarshi Mitra et al.

[ðŸ“„ View on arXiv](https://arxiv.org/abs/2507.12442v1)

### Abstract
Benchmarks and analyzes the performance of emerging SSM and SSM-Transformer hybrid architectures for language models with long context lengths. Focuses on system-level, device-aware evaluation and optimization, bridging architectural innovation and real-world scalable deployment. Accompanied by open-source benchmarking tools to facilitate reproducibility and further research.

---


## 3. Elk: Exploring the Efficiency of Inter-core Connected AI Chips with Deep Learning Compiler Techniques

**Authors:** Yiqi Liu et al.

[ðŸ“„ View on arXiv](https://arxiv.org/abs/2507.11506v1)

### Abstract
Examines system-level software and compiler optimizations for multi-core AI accelerators, focusing on deep learning workloads. Proposes techniques to improve memory layout, workload mapping, and resource utilization, underscoring the softwareâ€“hardware co-design needed for scalable and efficient ML deployment.

---

